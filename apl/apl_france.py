#!/usr/bin/env python
# coding: utf-8

# In[51]:


import pandas as pd
import numpy as np
import config
from sqlalchemy import text
import datetime
import random
print(config.version)
print(config.connection_string)
pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True


# In[3]:


def get_ps(year, specialite):
    sql=f"""
select ps.id, ps.nom, ps.prenom, an.dept_id, an.id adresse_norm_id, i.id iris, an.lon, an.lat from ps
join tarif t on t.ps_id = ps.id
join tarif_date_source tds on tds.tarif_id=t.id
join cabinet c on t.cabinet_id=c.id
join adresse_raw ar on c.adresse_raw_id=ar.id
join adresse_norm an on ar.adresse_norm_id=an.id
join specialite_profession sp on sp.profession_id=t.profession_id
join specialite s on s.id=sp.specialite_id
join iris.iris i on i.code=an.iris
where sp.specialite_id={specialite}
and tds.date_source_id >= {year}00 and  tds.date_source_id < {year+1}00
group by ps.id, c.id, an.id, i.id
"""
    # print(f"Quering PS for year {year} and specialite {specialite}")
    return pd.read_sql(sql, config.connection_string)


# In[4]:


def get_pa(year, specialite):
    sql = f"""
select pa.id, pa.nom, pa.prenom, an.dept_id, an.id adresse_norm_id, i.id iris, an.lon, an.lat 
from apl.ps_libreacces ps
join personne_activite pa on pa.inpp=ps.inpp
join pa_adresse_norm_date_source pands on pands.personne_activite_id=pa.id and pands.date_source_id=ps.date_source_id
join personne_activite_diplome pad on pad.personne_activite_id=pa.id
join diplome d on d.code_diplome=ps.code_diplome
join profession_diplome pd on pd.diplome_id=d.id
join profession p on p.id=pd.profession_id
join specialite_profession sp on sp.profession_id=p.id
join adresse_norm an on an.id=pands.adresse_norm_id
join date_source ds on ds.id=pands.date_source_id
join iris.iris i on i.code=an.iris
where sp.specialite_id={specialite}
and ps.code_mode_exercice='L'
and ds.annee={year}
and pands.adresse_norm_id is not null
group by pa.id, an.id, i.id
"""
    # print(f"Quering PA for year {year} and specialite {specialite}")
    # print(sql)
    return pd.read_sql(sql, config.connection_string)


# In[5]:


def get_by_source(year, specialite, source):
    if source=="PS":
        return get_ps(year, specialite)
    elif source=="PA":
        return get_pa(year, specialite)
    else:
        raise ValueError(f"Bad source: {source}")



# In[81]:


def get_pop_iris(year):
    yy=min(21, year)
    sql = f"""
select i.id iris, pi.iris iris_string, c.code code_commune, i.type type_iris, pi.pop, pi.pop0002, pi.pop0305, pi.pop0610, pi.pop1117, pi.pop1824, pi.pop2539, pi.pop4054, pi.pop5564, pi.pop6579, pi.pop80p
from iris.pop_iris pi
join iris.iris i on  pi.iris=i.code
join iris.commune c on i.commune_id=c.id
where year={yy}
"""
    # print(f"Get pop_iris for year {yy}")
    return pd.read_sql(text(sql), config.connection_string)


# In[85]:


def get_iris_matrix(time: int, time_type: str):
    sql = f"""
(select iris.id "iris1", iris.id "iris2", 0 "km", 0 "time_hc", 0 "time_hp" from iris.iris
union
select iris_id_from as "iris1", iris_id_to "iris2", route_km "km", route_min "time_hc", route_hp_min "time_hp" from iris.iris_matrix
where route_min <= {time}) order by "iris1", "iris2"
"""
    print(f"Get iris_matrix for time {time} {time_type}")
    m = pd.read_sql(sql, config.connection_string)
    m["time"] = m[f"time_{time_type.lower()}"].copy()
    return m


def get_iriss():
    sql = f"""
select i.id "iris", i.code "iris_string", i.nom "iris_label", c.dept_id "dept", c.code "code_commune", c.nom "commune_label", 20{year} "year", {specialite} "specialite" from iris.iris i
join iris.commune c on c.id=i.commune_id
"""
    # print(sql)
    return pd.read_sql(text(sql), config.connection_string)

# In[2]:


specialite=10
year=21
source="PA"
time=30
time_type="HC"
# accessibilite_exp=-0.12 #0.08 pour 45

iriss = get_iriss()
for time in [30, 45]:
    iris_matrix = get_iris_matrix(time, time_type)
    for time_type in ["HC", "HP"]:
        iris_matrix["iris"] = iris_matrix["iris2"].astype("int64")
        iris_matrix["time"] = iris_matrix[f"time_{time_type.lower()}"].copy()
        for source in ["PA", "PS"]:
            for year in range(20, 26):
                pop_iris = get_pop_iris(year)
                for specialite in range(1, 21):
                    accessibilite_exp = -(75 - time) * 4 / 1500
                    print(f"Compute APL in 20{year}, specialite {specialite} from {source} in {time}min {time_type}, e={accessibilite_exp}")
# In[84]:





                    # In[82]:


                    # pop_iris = get_pop_iris(year)
                    # pop_iris


                    # In[7]:


                    ps_df = get_by_source(year, specialite, source)
                    ps_df


                    # In[9]:


                    nb_ps = ps_df["id"].nunique()
                    nb_cabinet_ps = ps_df.groupby(["id", "lon", "lat"])
                    # print(f"Nb unique PS {nb_ps}")
                    # print(f"Nb cabinet {len(nb_cabinet_ps)}")


                    # In[10]:


                    ps_df["key"] = ps_df["id"].astype(str) + "_" + ps_df["lat"].astype(str) + "_" + ps_df["lon"].astype(str)
                    ps_df["nb_cabinet"] = ps_df.groupby("id")["key"].transform("nunique")
                    ps_df["nb_cabinet"].value_counts(normalize=True)


                    # In[11]:


                    ps_df["weight"] = (1 / ps_df["nb_cabinet"]).replace(np.inf, 0)
                    ps_df["nb"] = ps_df.groupby("iris")["weight"].transform("sum")
                    ps_df.head(10)


                    # In[12]:


                    ps_df = ps_df.sort_values(by='iris')
                    ps_df2 = ps_df.drop_duplicates(subset=['iris', 'nb'])
                    ps_df2


                    # In[15]:


                    iris_matrix_pop_df = iris_matrix.merge(pop_iris, on="iris", how="left", suffixes=('', ''))
                    iris_matrix_pop_df


                    # In[17]:


                    accessibilite_fn = lambda x: np.exp(accessibilite_exp * x)
                    iris_matrix_pop_df["accessibilite_weight"] = accessibilite_fn(iris_matrix_pop_df["time"])
                    iris_matrix_pop_df.head(10)


                    # In[18]:


                    cols = [col for col in iris_matrix_pop_df.columns if "pop" in col and col != "pop"]
                    cols


                    # In[78]:


                    yy=min(year, 24)
                    sql = f"""
                    select o.* from apl.overrepresentation o
                    join specialite s on s.psp_spe_snds=o.psp_spe_snds
                    where o.year={yy}
                    and s.id={specialite}
                    """
                    over = pd.read_sql(sql, config.connection_string)
                    over


                    # In[79]:


                    if len(over) > 0:
                        weights = over.values[0,2:]
                    else:
                        weights = np.ones(len(over.columns) - 2)
                    weights


                    # In[21]:


                    iris_matrix_pop_df["pop_gp"] = sum(w * iris_matrix_pop_df[c] for w, c in zip(weights, cols))
                    if specialite == 5:
                        iris_matrix_pop_df["pop_gp"] /= 2
                    iris_matrix_pop_df=iris_matrix_pop_df.sort_values(by='iris2')
                    # iris_matrix_pop_df.head(5)
                    if specialite != 5:
                        test_pop = iris_matrix_pop_df.drop_duplicates(subset=['iris2'])
                        ratio = test_pop["pop_gp"] / (test_pop["pop"] + 1)
                        ratio_mean = np.mean(ratio)
                        # print(ratio_mean)
                        iris_matrix_pop_df["pop_gp"] = iris_matrix_pop_df["pop_gp"] / ratio_mean


                    # In[22]:


                    matrix_df = iris_matrix_pop_df[["iris1","iris2","km","time","accessibilite_weight","pop_gp","pop","type_iris"]]
                    matrix_df.head(5)


                    # In[24]:


                    matrix_df["iris"] = matrix_df["iris1"].copy()
                    matrix_df = matrix_df.sort_values(by='iris2')
                    matrix_merge_df = matrix_df.merge(ps_df2, on="iris", how="left", suffixes=('', ''))
                    matrix_merge_df


                    # In[25]:


                    matrix_merge_df["nb"] = matrix_merge_df["nb"].fillna(0)
                    matrix_merge_df = matrix_merge_df.sort_values(by=['iris1', "iris2"])
                    matrix_merge_df["nb"].nunique()


                    # In[28]:


                    # matrix_merge_df = matrix_merge_df.sort_values(by='iris1')
                    matrix_merge_df["wpop"] = matrix_merge_df["accessibilite_weight"] * matrix_merge_df["pop_gp"]
                    matrix_merge_df["swpop"] = matrix_merge_df.groupby("iris1")["wpop"].transform("sum")
                    matrix_merge_df["R"] = (matrix_merge_df["nb"] / (matrix_merge_df["swpop"] / 100000)).replace(np.inf, 0)
                    matrix_merge_df.head(5)
                    # apl["R"].unique()


                    # In[29]:


                    rgp = matrix_merge_df[matrix_merge_df["iris1"] == matrix_merge_df["iris2"]]
                    rgp = rgp[["iris1", "iris2", "type_iris", "pop_gp", "nb", "R", "swpop", "wpop", "pop_gp", "pop"]].copy()
                    rgp


                    # In[30]:


                    apl = matrix_merge_df.merge(rgp, on="iris2", suffixes=("", "_dest"))
                    apl


                    # In[31]:


                    apl = apl.sort_values(by=['iris1', "iris2"])
                    apl["ap"] = apl["accessibilite_weight"] * apl["R_dest"]
                    apl.head(5)


                    # In[32]:


                    apl["apl"] = apl.groupby("iris1")["ap"].transform("sum")
                    apl


                    # In[33]:


                    apl2 = apl[apl["iris1"] == apl["iris2"]]
                    apl2


                    # In[34]:


                    # print(year, specialite, source)
                    apl2["apl"].describe()
                    # 21-10-PA:66-57


                    # In[ ]:


                    # 21 10 PA
                    # count    48569.000000
                    # mean        66.239008
                    # std         36.200523
                    # min          0.000000
                    # 25%         40.432111
                    # 50%         56.872725
                    # 75%         85.083597
                    # max        681.212229
                    # Name: apl, dtype: float64


                    # In[56]:


                    dico = {"year":year, "specialite_id":specialite, "source":source, "time":time, "time_type":time_type, "exp":accessibilite_exp}
                    dico["mean"] = np.mean(apl2["apl"])
                    dico["std"] = np.std(apl2["apl"])
                    dico["q10"], dico["q25"], dico["q50"], dico["q75"], dico["q90"] = np.quantile(apl2["apl"], [0.1, 0.25, 0.5, 0.75, 0.9])
                    dico["min"] = np.min(apl2["apl"])
                    dico["max"] = np.max(apl2["apl"])
                    dico["date"] = datetime.datetime.now()
                    dico["key"] = random.randint(0, 1000000000000)
                    dico


                    # In[62]:


                    study = pd.DataFrame(dico, index=[dico["key"]])
                    study


                    # In[35]:


                    apl2[apl2["apl"]>400]


                    # In[36]:


                    apl3 = apl2[["iris1", "type_iris", "nb", "apl", "ap", "accessibilite_weight", "wpop", "swpop", "R", "pop_gp","pop"]]
                    apl3 = apl3.rename(columns={'iris1': 'iris'})
                    apl3.head(5)


                    # In[37]:


                    # iriss = get_iriss()
                    # iriss


                    # In[38]:


                    apl_final = apl3.merge(iriss, on="iris", how="left", suffixes=("", "_dest"))
                    apl_final = apl_final[["year", "specialite", "iris", "iris_string", "type_iris", "nb", "apl", "ap", "accessibilite_weight", "R", "wpop", "swpop", "pop_gp","pop", "iris_label", "dept", "code_commune", "commune_label"]]
                    apl_final


                    # In[39]:


                    # d6=apl_final[apl_final["dept"]==6]
                    # d6
                    #
                    #
                    # # In[40]:
                    #
                    #
                    # d6["apl"].describe()
                    # # Ca change à cause des cabinets multi-dept
                    #
                    #
                    # # In[41]:
                    #
                    #
                    # apl_final.to_csv("apl_france.csv", index=False)


                    # In[65]:


                    apl_final["study_key"]=dico["key"]
                    study.to_sql("apl_study", config.connection_string, schema="apl", if_exists="append", index=False)
                    apl_final.to_sql("apl", config.connection_string, schema="apl", if_exists="append", index=False)



